{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics Overview\n",
    "\n",
    "Let's talk about some key statistical ideas. In this note section we will have a prief overview of statistical concepts. It will not be computationally rigorous, but hopelfully it will serve as a good refresher to some very important concepts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "Discrete Random Variable:\n",
    "\n",
    "Continuous Random Variable:\n",
    "\n",
    "Independant Random Variables:\n",
    "\n",
    "Expectated Value (E(x) or $\\mu$): Let $X$ be a random variable with a finite number of finite outcomes $x_{1}, x_{2}, \\dots, x_{k}$ occurring with probabilities $p_{1}, p_{2}, \\dots, p_{k}$ , respectively. The expectation of $X$ is defined as\n",
    "\n",
    "$${E} [X]=x_{1}p_{1}+x_{2}p_{2}+\\cdots +x_{k}p_{k}$$\n",
    "\n",
    "\n",
    "Moments: In mathematics, a moment is a specific quantitative measure, used in both mechanics and statistics, of the shape of a set of points.\n",
    "\n",
    "* 0th moment: is the total probability (or mass) i.e. 1 \n",
    "* 1st moment: is the mean\n",
    "* 2nd moment: relates to the varience\n",
    "* 3rd moment: relates to the skewness\n",
    "* 4th moment: relates to the kurtosis\n",
    "\n",
    "Varience:  Informally, Var(X) measures how far a set of (random) numbers are spread out from their average value. \n",
    "\n",
    "The variance of a random variable X is the expected value of the squared deviation from the mean of X:\n",
    "$$Var(X) = {E}\\left[(X-\\mu )^{2}\\right]$$\n",
    "$$Var(X) = {E}(X^2) - {E}(X)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Probability Mass Function (PMF)\n",
    "\n",
    "PMF is a function that gives the probability that a discrete random variable is exactly equal to some value. The probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete.\n",
    "\n",
    "Formally Suppose that $X: S\\rightarrow A(A\\subseteq R)$ is a discrete random Variable defined on a sample space S. Then the probability mass function $f_x: A\\rightarrow [0,1]$ for X is defined as:\n",
    "\n",
    "$$f_{X}(x)=\\Pr(X=x)=\\Pr(\\{s\\in S:X(s)=x\\})$$ \n",
    "\n",
    "Thinking of probability as mass helps to avoid mistakes since the physical mass is conserved as is the total probability for all hypothetical outcomes x: \n",
    "\n",
    "$$\\sum _{x\\in A}f_{X}(x)=1$$\n",
    "\n",
    "Here is a graph of a probability mass function. All the values of this function must be non-negative and sum up to 1:\n",
    " <img src=\"img/PMF.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Distribution Function (CDF)\n",
    "\n",
    "<img align= \"left\" src=\"img/CDF.png\" width=\"36%\">\n",
    "Every cumulative distribution function F is **non-decreasing** and **right-continuous**, which makes it a càdlàg function. Furthermore,\n",
    "\n",
    "$$\\lim _{x\\to -\\infty }F(x)=0,\\quad \\lim _{x\\to +\\infty }F(x)=1.$$\n",
    "\n",
    "The CDF of a continuous random variable X can be expressed as the integral of its probability density function ƒX as follows:\n",
    "\n",
    "$$F_{X}(x)=\\int _{-\\infty }^{x}f_{X}(t)\\,dt$$\n",
    "\n",
    "In the case of a random variable X which has distribution having a discrete component at a value b,\n",
    "\n",
    "$$\\operatorname {P} (X=b)=F_{X}(b)-\\lim _{x\\to b^{-}}F_{X}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density Function\n",
    "\n",
    "a function of a **continuous random variable**, whose integral across an interval gives the probability that the value of the variable lies within the same interval.\n",
    "\n",
    "<img align= \"left\" src=\"img/PDF.png\" width=\"45%\">\n",
    "Suppose a species of bacteria typically lives 4 to 6 hours. What is the probability that a bacterium lives exactly 5 hours? The answer is 0%. A lot of bacteria live for approximately 5 hours, but there is no chance that any given bacterium dies at exactly 5.0000000000... hours.\n",
    "\n",
    "Instead we might ask: What is the probability that the bacterium dies between 5 hours and 5.01 hours? Let's say the answer is 0.02 (i.e., 2%). Next: What is the probability that the bacterium dies between 5 hours and 5.001 hours? The answer is probably around 0.002, since this is 1/10th of the previous interval. The probability that the bacterium \n",
    "dies between 5 hours and 5.0001 hours is probably about 0.0002, and so on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation vs. Causation\n",
    "\n",
    "<img src=\"img/Cheese-Bedsheets.jpg\" height=\"110%\">\n",
    "In statistics, correlation can be quantified and given a number where zero is “no correlation” and 1 is “perfect correlation.” Perfect correlation exists and it is pretty much indistinguishable from causation. You’ll rarely (if ever) use the term “causation” and instead you’ll be talking about various types of correlation coefficients (A correlation coefficient is a number that quantifies a type of correlation and dependence, meaning statistical relationships between two or more values in fundamental statistics.) and whether your results are statistically significant.\n",
    "\n",
    "Causation can be extremely hard to prove, as what you’re trying to prove is 100 percent correlation (which rarely happens). Take the case of cigarette smoking. For decades, activists, trade groups, and scientists debated about whether tobacco smoke caused lung cancer and if so, how strong was the link. Many other reasons were suggested for the link between lung cancer and smoking, including sleep deprivation or alcoholism. In layman’s terms, it’s now known that smoking causes lung cancer. But in scientific (or statistical) terms, you can’t really say “cause” as that would mean every single person who smoked even just one cigarette would get lung cancer. As statisticians, we say that there is a very strong correlation between smoking and lung cancer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "In probability theory, the central limit theorem (CLT) establishes that, in most situations, when independent random variables are added, their properly normalized sum tends toward a Normal distribution (informally a \"bell curve\") even if the original variables themselves are not normally distributed. The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.\n",
    "\n",
    "<img src=\"img/CLT.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "A statistical hypothesis is an assumption about a population parameter. This assumption may or may not be true. Hypothesis testing refers to the formal procedures used by statisticians to accept or reject statistical hypotheses.\n",
    "\n",
    "#### Statistical Hypotheses\n",
    "The best way to determine whether a statistical hypothesis is true would be to examine the entire population. Since that is often impractical, researchers typically examine a random sample from the population. If sample data are not consistent with the statistical hypothesis, the hypothesis is rejected.\n",
    "\n",
    " There are two types of statistical hypotheses.\n",
    "\n",
    "* **Null hypothesis**. The null hypothesis, denoted by H0, is usually the hypothesis that sample observations result purely from chance. \n",
    "\n",
    "* **Alternative hypothesis**. The alternative hypothesis, denoted by H1 or Ha, is the hypothesis that sample observations are influenced by some non-random cause.\n",
    "\n",
    "#### Hypothesis Tests\n",
    "Statisticians follow a formal process to determine whether to reject a null hypothesis, based on sample data. This process, called hypothesis testing, consists of four steps.\n",
    "\n",
    "* State the hypotheses. This involves stating the null and alternative hypotheses. The hypotheses are stated in such a way that they are mutually exclusive. That is, if one is true, the other must be false. \n",
    "\n",
    "* Formulate an analysis plan. The analysis plan describes how to use sample data to evaluate the null hypothesis. The evaluation often focuses around a single test statistic. \n",
    "\n",
    "* Analyze sample data. Find the value of the test statistic (mean score, proportion, t statistic, z-score, etc.) described in the analysis plan. \n",
    "\n",
    "* Interpret results. Apply the decision rule described in the analysis plan. If the value of the test statistic is unlikely, based on the null hypothesis, reject the null hypothesis. \n",
    "\n",
    "#### Decision Errors\n",
    "Two types of errors can result from a hypothesis test.\n",
    "\n",
    "* Type I error. A Type I error occurs when the researcher rejects a null hypothesis when it is true. The probability of committing a Type I error is called the significance level. This probability is also called alpha, and is often denoted by α.\n",
    "\n",
    "* Type II error. A Type II error occurs when the researcher fails to reject a null hypothesis that is false. The probability of committing a Type II error is called Beta, and is often denoted by β. The probability of not committing a Type II error is called the Power of the test.\n",
    "\n",
    "<img src=\"img/error.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "\n",
    "Statisticians use a confidence interval to describe the amount of uncertainty associated with a sample estimate of a population parameter.\n",
    "\n",
    "#### How to Interpret Confidence Intervals\n",
    "Suppose that a 90% confidence interval states that the population mean is greater than 100 and less than 200. How would you interpret this statement?\n",
    "\n",
    "Some people think this means there is a 90% chance that the population mean falls between 100 and 200. This is incorrect. Like any population parameter, the population mean is a constant, not a random variable. It does not change. The probability that a constant falls within any given range is always 0.00 or 1.00.\n",
    "\n",
    "A correct way to interpret this result would be to say: There is a 90% cahnce that the true population mean $\\mu$ lies between 100 and 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axioms of Kolmogorov\n",
    "\n",
    "Let S be a sample space and $\\mathbb{P}: \\mathcal P (S) \\rightarrow \\mathbb{R}$. We say that $\\mathbb{P}$ is a proability measure and $(s, \\mathbb{P})$ is a probability space if:\n",
    "\n",
    "* $\\mathbb{P}(A) \\geq 0$ for every event A from S. (non-negativity)\n",
    "\n",
    "* $\\mathbb{P}(s) = 1$ (unity)\n",
    "\n",
    "* If $A_1, A_2, \\dots $ is a sequence of mutually exclusive events then: $$\\mathbb{P}\\left(\\bigcup _{i=1}^{\\infty}A_{i}\\right)=\\sum _{i=1}^{\\infty }P(A_{i})$$\n",
    "\n",
    "#### Consequences\n",
    "\n",
    "From the Kolmogorov axioms, one can deduce other useful rules for calculating probabilities.\n",
    "\n",
    "The probability of the empty set: $$\\mathbb{P}(\\emptyset)=0$$\n",
    "\n",
    "Monotonicity: $${\\text{if}}\\quad A\\subseteq B\\quad {\\text{then}}\\quad P(A)\\leq P(B)$$\n",
    "\n",
    "Addition Lay of Probability: $$P(A\\cup B)=P(A)+P(B)-P(A\\cap B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "\n",
    "Given two events A and B, from a probability space, with P(B) > 0, the conditional probability of A given B is defined as the quotient of the probability of the joint of events A and B, and the probability of B:\n",
    "\n",
    "$$P(A|B)={\\frac {P(A\\cap B)}{P(B)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of Total Probability\n",
    "\n",
    "Suppose we have a sequence of sets $B_1, \\dots, B_n$ (countably infinite or finite) on a probability space $(s,\\mathbb{P})$ satisfying:\n",
    "\n",
    "1. $B_i\\cap B_j = \\emptyset$ (mutually exclusive)\n",
    "\n",
    "2. $\\bigcup _{i=1}^{\\infty \\text { or } n }B_{i} = S$ (covering of the space)\n",
    "\n",
    "3. $\\mathbb{P}(B_i)>0$ (nontrivial)\n",
    "\n",
    "\n",
    "We say the $B_i$'s constitute a covering or partition or decomposition of S.\n",
    "\n",
    "**Law of total Probability**: If $B_1, \\dots, B_n$ is a covering of S and A is any event, then $$\\mathbb{P}(A)=\\sum _{i=1}^{n}\\mathbb{P}(A\\cap B_{i})$$ or alternatively $$\\mathbb{P}(A)=\\sum _{i=1}^{n}\\mathbb{P}(A\\mid B_{i})\\mathbb{P}(B_{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "Suppose that $B_1, \\dots, B_n$ is a partition of S:\n",
    "\n",
    "$$\\mathbb{P}(B_k | A) = \\frac{\\mathbb{P}(A|B_k)\\mathbb{P}(B_k)}{\\sum_{i=1}^{\\infty}\\mathbb{P}(A | B_i) \\mathbb{P}(B_i)}$$\n",
    "with conditional probability:\n",
    "$$\\mathbb{P}(B_k | A) = \\frac{\\mathbb{P}(A\\cup B_k)}{\\sum_{i=1}^{\\infty}\\mathbb{P}(A\\cup B_i)}$$\n",
    "from Kolmogorav's 3rd axiom:\n",
    "\n",
    "$$\\mathbb{P}(B_k | A) = \\frac{\\mathbb{P}(A\\cup B_k)}\n",
    "{\\mathbb{P}(\\bigcup_{i=1}^{\\infty}{A\\cup B_i})}$$\n",
    "\n",
    "$$\\mathbb{P}(B_k | A) = \\frac{\\mathbb{P}(A\\cup B_k)}{\\mathbb{P}(A)}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 w/Conda",
   "language": "python",
   "name": "ipykernal_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
